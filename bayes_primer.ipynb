{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "perceived-motivation",
   "metadata": {},
   "source": [
    "# Bayesian probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-dream",
   "metadata": {},
   "source": [
    "## Useful links:\n",
    "\n",
    "- Aki Vehtari's course: https://github.com/avehtari/BDA_course_Aalto\n",
    "\n",
    "- Great intro on Bayesian probability: https://m-clark.github.io/bayesian-basics/intro.html\n",
    "\n",
    "- Differences between Bayesian and Frequestist approaches: http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/\n",
    "\n",
    "- Bayesian data analysis (free): http://www.stat.columbia.edu/~gelman/book/\n",
    "\n",
    "- Bayesian cognitive modeling book (first 2 chapters are free): https://bayesmodels.com/\n",
    "\n",
    "- Doing Bayesian data analysis book: https://sites.google.com/site/doingbayesiandataanalysis/home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-aside",
   "metadata": {},
   "source": [
    "## Bayesian vs. Frequentist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-subdivision",
   "metadata": {},
   "source": [
    "Until now, you have seen different methods to estimate parameters using a frequentist approach (maximum likelihood estimation, **MLE**).\n",
    "\n",
    "For a given set of data, using MLE, you estimate one set of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "numeric-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "magnetic-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.normal(0, 1, 100)\n",
    "X = sm.add_constant(x)\n",
    "y = 2 + 1.2*x + np.random.normal(0, 1, size=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thick-minority",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/laurafontanesi/miniconda3/envs/stanenv/lib/python3.7/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdx0lEQVR4nO3df2xkV3UH8O+ZH54Z2+O1l13nl71KTBOcEIUEFpoUtKxCVG1KlIiISkSC0pZq9w/SUEQhoYRIDW0hpUJFKmp3BahCpKEogJK2sGmq4C5VE5pNGmjCmmxwEvZHdm3veu2xx/P79I+Z8Y698+ON571334/vB0XJemfHd5xwz7vnnnuuqCqIiCh8IqYHQEREZjAAEBGFFAMAEVFIMQAQEYUUAwARUUjFTA+gG3v27NGDBw+aHgYRkd9Isy/6agUwPz9veghERIHhqwBARET2YQAgIgopBgAiopBiACAiCikGACKikPJVGSgRUdhMTc9i/6EZHFvIYnykH/t2TWD35Kgt780VABGRR01Nz+KBx1/CbCaH4VQcs5kcHnj8JUxNz9ry/gwAREQetf/QDOJRQX9fDCLVv8ejgv2HZmx5fwYAIiKPOraQRSoeXfe1VDyK4wtZW96fAYCIyKPGR/qxWiyv+9pqsYyxkX5b3p8BgIjIo/btmkCxrMgWSlCt/r1YVuzbNWHL+zMAEBF51O7JUTx4+1sxmk5icbWI0XQSD97+VtuqgFgGSkTkYbsnR22b8DfiCoCIKKQYAIiIQooBgIgopBgAiIhCigGAiCikGACIiEKKAYCIKKQYAIiIQooBgIgopBgAiIhCigGAiCik2AuIiALFySsUg4YrACIKDKevUAwaBgAiCgynr1AMGgYAIgoMp69QDBqjAUBEhkXkURGZFpEjInKTyfEQkb85fYVi0JheAXwVwEFVnQTwNgBHDI+HiHzM6SsUg8ZYFZCIDAHYBeD3AUBVCwAKpsZD5FWsarFu9+QoHkR1L+D4QhZj/Hm1Japq5huLXA/gAIBfoPr0/xyAT6jqyobX7QWwFwB27Njxjtdff93lkRKZU69qiUcFqXgUq8UyimW19V5YCgVp9kWTKaAYgLcD+HtVvQHACoD7Nr5IVQ+o6k5V3bl9+3a3x0hkFKtayEkmA8BxAMdV9ae1Xz+KakAgohpWtZCTjAUAVT0F4JiIvKX2pfehmg4iohpWtZCTTFcB/TGAh0Xk5wCuB/BXZodD5C2saiEnGe0FpKovANhpcgxEXsaqFnISm8ERedzuyVFO+OQI0ykgIiIyhAGAiCikGACIiEKKewBEAcY2EtQOVwBEAcXLUagTBgCigGIbCeqEAYAooNhGgjphACAKKLaRoE4YAIgCim0kqBMGAKKA2j05igdvfytG00ksrhYxmk7yHgFah2WgRAHGNhLUDlcAREQhxQBARBRSDABERCHFAEBEFFIMAEREIcUAQEQUUgwAREQhxQBARBRSPAhGFDK8I4DquAIgChHeEUCNuAIg8qHNPsU33hEAAP19MWQLJew/NMNVQAhxBUDkM708xfOOAGrEAEDkM73c9MU7AqgRAwCRz/TyFM87AqgRAwCRz/TyFM87AqgRN4GJfGbfrgk88PhLyBZKSMWjWC2Wu3qK5x0BVMcVAJHP8Cme7MIVAJEP8Sme7MAAQESew9PK7mAKiIg8haeV3cMAQESe0ss5B+oOU0BELmJqo7NjC1kMp+LrvsbTys7gCoDIJUxtWMPTyu5hACByCVMb1vC0snuMp4BEJArgMIATqnqb6fEQOSWIqQ0nUlq7J0fxIKoB8/hCFmNMlTnGeAAA8AkARwAMmR4IkZPGR/oxm8mttWIG/J3aqKe04lFZl9J6ELAlCHDCd57RFJCIjAF4P4CvmxwHkRuCltpgSsv/TK8A/hbAZwCkW71ARPYC2AsAO3bscGdURE30mu4IWmojiCmtsDEWAETkNgCzqvqciOxu9TpVPQDgAADs3LlT3Rkd0Xp2pTuClNoIWkorjEymgN4N4HYReQ3AdwDcLCLfNjgeopaY7rhQ0FJaYWQsAKjqZ1V1TFUvB/AhAE+p6odNjYeoHV6leCF2JfU/03sA5BNeOMFqcgxMdzQXpJRWGHniIJiqTvEMgHd54QSr6TEw3UFBxBUAddSY/waA/r4YsoUS9h+ace3pz9QYGlcdg31RiAgWV4u+r+AhAhgAyAIvlPvZNYZu0kgbK3+qVy9W8IU7rg3kxO+FNB+5iwGAOvJC/tuOMXRbyml65WPXhGzlfZw81Uve5Yk9API2L+S/7RhDt6WcJit/ut3zmJqexV0HnsF7HnoKdx14Zu11Vt+HZa7hxBUAdeSFE6ybHUPj0+9cJo+LhxLrfr/dhO7EysfqU303q492T+9W38cLaT5yHwMAWeKFcr9ux7BxYpxfzuPEuRxEBOlkdbJrN6Hv2zWBBx5/CdlCCal4tLYHsPmVTzdplm4m5HaTvNX38UKaj9zHFBAF1sa0xkXpJADg1GLOUhrJ7oNO3aRZurkUpV2qyur7eCHNR+7jCoACa+PT71AqDkBxailvuZTTzpVPN0/13aw+2j29W30fL6T5yH0MABRYzSbGWDSCt+8YwSN7b/TEeFo91XczIbeb5Lt5Hy+k+chdouqfBps7d+7Uw4cPmx4G+URjzr1xYjTVr8bJ8dQ3l/n0Ti1I0y8yAFCQdTsxOn0YihM1GcIAQMFj54TttRUDkY2aBgDuAZBv2X161fTJ307YqoHsxjJQ8i27T696uee/6W6oFEwMAORbdk/Y3dTeu42tGsgJDADkW3ZP2F4+DOXl1Qn5FwMA+dLU9CwWVvJ47cwKjp7OYGm10POE7eUrDr28OiH/4iYw+U7j5u/YcAqnM3kcP5fDVaOD+Pz7J3uasDd7GKrbewa63cy1uy8REcAAQD60sVpnKNWHbKGE4f4+4we8OlUj1V9bLJexmC3ijcVVPP/rBXx895txzy1XtfwebNVATmAAIF/otq1z/fUvn15Csazoi0Vw5WjakUmzm/LR/YdmUCyXcWa5CBEgHo2gXFF8bepXuG5s2LW+REQA9wDIBzaWQIoAJ87lkMkV117TmA+vv/7V+WUs5UpYLVaftl87s+xI6WQ3G7THFrJYzFYn/4gIBIKoCMoVZUUPuY4BgNa0ulXKtG7bOtdfn8mVEIEgFokgEhEsrZZsL52cmp7F0moR06cymJlbxtJqNSi12qAdH+lHvlyBNJzLVAUSsQgresh1DAAEwNsHjTY+YQ+l4rhsOAkFmlbr1F9faJhoRYBCuWJr6WT9ZzaQiEJQff+Ti6uYy+RabtDu2zWBWKSa9lFVVCqKChTpZIwVPeQ67gEQAG+3QejU1rme77//sRcxPtKPdCKG1WIZfdEISmWFSPUpuy8a6al0cmP1zsJKHvGoYEsqiUQsirlMHrlSGdlCGV+687qWLZc/vvvN+NrUr1CqKBKxCNLJOPpiUVb01LDlhXs6rgBE5G4RGXFjMGSOlw8atTug1WzlMrecx9JqEelkDBUoSpUKKhXFUCq26dLJZt/n6NwySuUKACCdjGNi+yCuvngIW1LxthPWPbdchf0ffgfeeflWbB3owxXbBj1z3sA0L69Eg8jKCuBiAM+KyPMAvgngCfVTC1GyxMt3wrYrgbzrwDMXrFwAIB4RjAwkUCovYaVQRqmiOLtSxMS2vk2NodkKKR6J4HQmj6HU+fe0+jNjRU9zXl6JBlHHAKCq94vI5wH8NoA/APB3IvJdAN9Q1V85PUByh9cPGrWaMFtds7i4WsTBT97YtMXzZjqGHp3NIJsvoVhR9EUj2J5O4KKhBI6fy3n2Z+ZH3VybSb2ztAlce+I/VfurBGAEwKMi8tcOjo1c5OU2CO10apFgRxO1qelZZHLVyT8aEZQqipPnciiUK7hqdNB3PzMvY8sLd3VcAYjIPQA+CmAewNcBfFpViyISAXAUwGecHSK5xY9piU4rFzueKPcfmsFIfxxnVgrQSrWiqIJqSumLH2i+2Uub4/WVaNBY2QPYBuBOVX298YuqWhGR25wZFpE1nVokdNrbsFJxcmwhi22DCSRiUcwv51EoV9AXjSAVj6y91onKlTBWw7Dlhbt4JSQFWrtrHgFYugLyrgPPXBBEsoUSRtPJtTJUO6+SnJqexUMHp/Hy7DLiUcFF6QRi0Qivp6ReNL0SkgfBKNCa7W188O2XYf+hGez79nOYzeRqZwVa7w90uifAzstazrexWEFUAK0AJxerY+QFMGQ3HgSjwGvc22h8Wi9XKoiI4OTiKoDqCeNm+wOd0hJ2Vq7Ug0lZqxvOAgEqwPxyHldsG2A1DNmKAYBCpfFpPRGLolRRiFYn2KFUvGXFSbsNcjvPUNSDSV80Uh2bnG9jwWoYshsDgCFh3ODzgsan9e3pBE6eywFQFMrdXQHZ+O8vnYhhsdYErtfKlXowqY+tgmrPoGhEWA1DtmMAMKCbC0Soyq6A2fi0nk7GcelwrasogNF00tL7Tk3P4k8f/RmW8yWUK4r5iKAvIogP9GFxtdhT5Uq9DDIeFVyyJYHTS3mUVDGxdQD33Xo1//sgWxkLACIyDuBbqLaaqAA4oKpfNTUeN/G4e3fsDJgb68yjEcHo0IUHuNoFnC/96AjOZYuISrWXv1aAbK0n0E/uvbmnz7pxv+GGHSNcHZJjTK4ASgA+parPi0gawHMi8qSq/sLgmFzB4+7dWdsYrShenV9BoVxBVAQPHZzuemK0UmfeKeC8eiaLiACRSLWyTgTQiuLVM/b8+/PjgTzyJ2MBQFXfAPBG7Z8zInIEwGUAAh8AvNx4zQm9pm+OLWQRFeCNxTxEgGhEUKkoXp5dxtT07KaCQLs/wxUahYUnzgGIyOUAbgDw0ya/t1dEDovI4bm5OdfH5oROdeVBYkd73/GRfpxeyq+7RlEgbevi291u1unms3atsaemZxEVQaGsyBXLKFcqqKiiosDEtoEufjJE5hk/CSwigwD+E8Bfqur32702SCeB60/FQT/u3ukUrRVT07P42LeeRVQEERGoVnvxXLoliYqez7vXf6ZHZzPI5EoY6Y9j22Ci69O/rcbcF41gpVBGoVTG3HIetbQ/+qKCLak4vvzBtwXy3yEFQtOTwEargEQkDuB7AB7uNPkHTVjyvHbsd+yeHMWV2wfx2tksyrV2zNsGk4hFBaO1+4Eb8/bZfAkVVZxZKSARi2IoFV9L4QDomN5p1ZAsHtF1N4DNL+eRL1WQjEc5+ZMvmawCEgDfAHBEVb9iahzkrF72OzbW2g8mYthSO627sda+MW9fb9uslfMHvOpBR4GOAanVRvH9j7249meHUnEMpeJQVSyuFjn5ky+ZXAG8G8BHAPyfiLxQ+9qfqeoPzQ2J7NZNe9/GCX+wL4ozKwUMpeIYrp3QFVRv+mpWa9+40mg8RVuo5Wkag46VgNRshTZ+KFyb9xR8JquA/gst8lIUHFbb+24svXxlbhmlsmIgcb7BGgCMDCRw8JMX7h00rjQaT9H2RSMXbLJvtt88e9VT0PAkMDnOyn7Hxlr/XLH65H5qMYd0svpk327vYN+uCXz60Z/hxMIqSrUmb9U/E7nghO9m+82zVz0FDQMAecLGWv+IABUFcqUKMrki0snWjdrqFAAEEBFEIoLBRAx/02RztpcNeCc279kXikxhACDb9DKRjY/0439/vbBW6x8VQaVWojy7lOvYDG3/oRlsScVxyZbU2tf8cHiLfaHIJAaAgDD9FNnrRLZv18Rarb+iGgQioohHI8iXtWOjtm7KTU3/rBrx1DGZ5ImTwNQbO07b9qrXW7Hqtf6RSPUylFhUMD7Sj7GRFN51+VY8svfGthPi+Eg/VovltV8vrRbxytwyZjP5dad9vfCzatTu1DGR0xgAAsCuKwk7tUhop5uJrNX3ue/WqzGaTmLH1n5csW0Asaj1HviN7TWWVgs4cW4VpbLi4qHEuknezusb7bAxcAEsLSX3MAAEQKvJ9+hsxvKE3uuTsdWJrN33aXZ/r9VL0Bv/7KmlPGIRwdhICkOpvnWTvNeeuMPUF4q8x3gvoG4EqReQnZr1rplfzuHsShFjI6mWPW+avUeprJhfzldbLkcEl2/tx8FPvrfjGBr3ANp9Pzt6A3XynoeewnAqDpHzx0zqJ3bHmpxMrn//fbsmjOwNhKUvFBnlvV5AZI9mB5TOrhQx0h+3vLlYL8M8uZhDBLUqnIri6Jy1lstWa+TduAuhXfuJVoe5bprYankT2+5N5LD0hSLvYQooAJqlTgYTUWwbTKx7XbuJdnykH6czeURQraEXqf4Vj0S6zo+3W1M2pooyuSJm5pZx5NQSFleLtm3EtkurtEozPT1z1tLegNc2kYl6wQAQELsnR/HI3hvxk3tvxiN7b8RVFw11tblYnzTr/6uoQhW4aChh6enc6sRY/z7zyzmcWFhFoVw9tdvfF7VtIm01yQPVFNT9j70IAPjCHdeuVRdZ3Rvw2iYyUS8YAAKq283F3ZOjuGp0EBGptmOIRQSXDicRi0YsVaRYnRjrk/NKvgxFtXHbpVtS2J5O2jqRbgyIANoGKKub2F7bRCbqBQNAQG2moubePZMYHTpfhtl4+raXW7SajW0oFcfkxWlMbB/EUKpzr59edQpQVgMmyzYpSLgJHGDdbi622sgF0HGDtNu+/xtfn8kVcWoxB0U1TWN3JUynzWerm9jsCEpBwjJQ6shK6abVMtC6xteXyhWcOJcDAFxWSzu1+7NOfQarWLZJPtS0DJQpIOrISnqn25TTBQe3ooLLhi88uGUXOw9c7Z4cxb5dExgb6cexhSz2H5phFRD5ElNA1JHV9M5mUk67J0ebHtyyez/Azl7+7OBJQcEAQB05nfceH+nHa2eWsbRaQqFcQV80gqFUDJe/adCW96+z68BVpw6eXuo2StQOU0DUUS89eqy4aWIrZjOF2pmA6j2+s5kCbprYasv7N+ql4V1du5QYD4qRn3AFQJY42a7g6Zmz2D7Yh0zu/AognYzh6ZmzuMfG72NX6qZdSoz9/clPGABoU6ykOaymQo4tZLFtMIHt6eTa11TV9jMBdk3O7VJi9z/2ouO9jojswhQQdc1KmqObVIhbh6vsOsXbLiXGg2LkJ1wBBJDTm5BWnqS7edp263BVt4fV2mmVEuNBMfITBoCAcaJEcWNAefn0Ei7ZkkImV8Rcpnp3QDwiWFwtrv2Zbto+21mi2Y4bk7Nbn4XIDgwAAVGfpJ//9QIEwMVbkms9b3rZhGwWUJbzZZw4l8VKvgIRVHsGVRSZXGnt7oBun7bd6Inv1uTM/v7kFwwAAdA4SVdUIQBOnsvh0mEgnYz3tAnZLJWzdSC+du1iBAKtVF870h9fCzReSIW0SoVxciaq4iZwADRO0n3RCAQCEWAukwfQ2yZks43TNw0kIKi2ci6rIhYVXLolhW2DifXN1Rw8O9AJ6/GJOuMKIAAa8+3bBhM4ubgKUSBfqvR8yXirVM5gIobt6cQFzdUaA43Jp23W4xN1xhVAADSWHg6l4rh0SwqRiCAaifT85N2qidofvecK25qrOYEXtxB1xhVAAGzMt8eiYlvKpd3G6XVjw56tdrGz5JMoqHgfQEC061EfxuZkG+8nmF/OYyFbRDoZw5Wj6VD8DIgaNL0PgAEg4Lq9qMXK+/klmNTHevT0EjL5MrYOxPGmgUTPPwMiH+KFMGFk9bJ2K0xU1vTSvbN+MfyVFw1hbCSFbYPJnn8GREHCAOAxdrQrbmTnZqidwcQKuwION4SJmmMA8BAnnrDtbE7m9kRqV8Bhgzai5owGABHZIyK/FJFXROQ+k2PxArufsKemZ3EuW8BrZ7I4OpvB0mqhp3JNtydSuwKOnfcBEwWJsQAgIlEAXwNwK4BrANwlIteYGo8X2PmEXV9NFMoVjA0nAQWOn1tFPCKb3vx0eyK1K+CYPpVM5FUmzwG8C8ArqjoDACLyHQB3APiFwTEZZWft+saTsEOpPmQLJYwMJDY98bnd6dLOfkLsAUR0IZMB4DIAxxp+fRzAbxoaiyfYOeF10465G25OpGytTOQskwGgWV3qBYcSRGQvgL0AsGPHDqfHZJSdE15QTsLyyZ3IOSYDwHEA4w2/HgNwcuOLVPUAgANA9SCYO0Mzx64JzwvtmInI20xWAT0L4EoRuUJE+gB8CMDjBscTKNz4JKJOjK0AVLUkIncDeAJAFMA3VfUlU+MJok6rCT+1dSAi+xntBqqqPwTwQ5NjCCsn7g4mIn/hSeCQcrutAxF5DwNASLE/DhHxQhiPcDsf30uZKPcOiIKBKwAPMNFmebNtHZweq93dUImoNQYADzCRj99smaiTYzURCInCjCkgD3CqbUMnmzl05uRYN/Yv6u+LIVsoYf+hGaaYiBzAFYAH+KlfvZNj5cY0kbsYADzAT/3qnRyrnwIhURAwAHjAZvPxJjZMnWwx4adASBQEouqf/mo7d+7Uw4cPmx6GJzSe5G1s9ub3fj/1ElO2fyayVbPuy9wE9qugbpiy/TORe5gC8ilumBJRrxgAfIobpkTUKwYAn+KGKRH1insAPmX3fbns70MUPqwCcpFXJ9mgVhQR0ZqmVUBMAbnEy31ueDcAUTgxALjEy5MsK4qIwokBwCVenmRZUUQUTgwALvHyJMuKIqJwYgDokdV+PF6eZJ3s70NE3sUqoB50Wz3DPjdEZAh7Admt23487HNDRF7CFFAPvLyxS0TUCQNAD7y8sUtE1EmoAoDdF6h4eWOXiKiT0AQAJ07isnqGiPwsNJvATl2gwo1dIvKr0KwAuGFLRLReaAIAN2yJiNYLTQDghi0R0XqhCQDcsCUiWi/wm8BevYSFiMi0QK8AvHwJCxGRaYEOAF6+hIWIyLRABwCWfhIRtWYkAIjIl0VkWkR+LiI/EJFhJ74PSz+JiFoztQJ4EsC1qnodgJcBfNaJb8LSTyKi1owEAFX9d1Ut1X75DIAxJ74PSz+JiFozfiOYiPwLgH9W1W+3+P29APYCwI4dO97x+uuvuzk8IqIgaHojmGMBQET+A8DFTX7rc6r6WO01nwOwE8CdamEgXrsSkojIJ9y9ElJVb2n3+yLyUQC3AXiflcmfiIjsZeQksIjsAXAvgPeqKmsyiYgMMFUF9HcA0gCeFJEXROQfDI2DiCi0jKwAVPU3THxfIiI6z3gVUDdEZA6AX8qAtgGYNz0IGwThc/AzeEcQPocfP8O8qu7Z+EVfBQA/EZHDqrrT9Dh6FYTPwc/gHUH4HEH4DHWB7gVEREStMQAQEYUUA4BzDpgegE2C8Dn4GbwjCJ8jCJ8BAPcAiIhCiysAIqKQYgAgIgopBgAHuXXxjZNE5HdF5CURqYiIr0rfRGSPiPxSRF4RkftMj2czROSbIjIrIi+aHstmici4iPxYRI7U/lv6hOkxbYaIJEXkf0TkZ7XP8eemx9QrBgBnuXLxjcNeBHAngEOmB9INEYkC+BqAWwFcA+AuEbnG7Kg25R8BXHCAx2dKAD6lqlcDuBHAx3367yIP4GZVfRuA6wHsEZEbzQ6pNwwADnLr4hsnqeoRVf2l6XFswrsAvKKqM6paAPAdAHcYHlPXVPUQgLOmx9ELVX1DVZ+v/XMGwBEAl5kdVfe0arn2y3jtL19X0TAAuOcPAfzI9CBC5DIAxxp+fRw+nHSCRkQuB3ADgJ8aHsqmiEhURF4AMAvgSVX15eeoM9IMLki6uPimBOBhN8dmlZXP4EPNLsDw9dOa34nIIIDvAfgTVV0yPZ7NUNUygOtr+3k/EJFrVdW3+zMMAD0KwsU3nT6DTx0HMN7w6zEAJw2NJfREJI7q5P+wqn7f9Hh6parnRGQK1f0Z3wYApoAc1HDxze28+MZ1zwK4UkSuEJE+AB8C8LjhMYWSiAiAbwA4oqpfMT2ezRKR7fVKPhFJAbgFwLTRQfWIAcBZvr/4RkQ+ICLHAdwE4N9E5AnTY7Kitvl+N4AnUN10/K6qvmR2VN0TkUcAPA3gLSJyXEQ+ZnpMm/BuAB8BcHPt/wcviMjvmB7UJlwC4Mci8nNUHzCeVNV/NTymnrAVBBFRSHEFQEQUUgwAREQhxQBARBRSDABERCHFAEBEFFIMAEREIcUAQEQUUgwARD0QkXfW7ntIishArU/8tabHRWQFD4IR9UhE/gJAEkAKwHFV/aLhIRFZwgBA1KNar6FnAeQA/FatYySR5zEFRNS7rQAGUe37lDQ8FiLLuAIg6pGIPI7qjWNXALhEVe82PCQiS3gfAFEPROT3AJRU9Z9q9xD/t4jcrKpPmR4bUSdcARARhRT3AIiIQooBgIgopBgAiIhCigGAiCikGACIiEKKAYCIKKQYAIiIQur/ARwOduVPVAZ5AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.regplot(x, y, fit_reg=False)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "sns.despine();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "inside-territory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.630\n",
      "Model:                            OLS   Adj. R-squared:                  0.626\n",
      "Method:                 Least Squares   F-statistic:                     166.9\n",
      "Date:                Sat, 20 Feb 2021   Prob (F-statistic):           6.90e-23\n",
      "Time:                        20:21:22   Log-Likelihood:                -141.82\n",
      "No. Observations:                 100   AIC:                             287.6\n",
      "Df Residuals:                      98   BIC:                             292.8\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.9939      0.101     19.719      0.000       1.793       2.195\n",
      "x1             1.2372      0.096     12.921      0.000       1.047       1.427\n",
      "==============================================================================\n",
      "Omnibus:                        2.661   Durbin-Watson:                   1.783\n",
      "Prob(Omnibus):                  0.264   Jarque-Bera (JB):                2.111\n",
      "Skew:                           0.338   Prob(JB):                        0.348\n",
      "Kurtosis:                       3.223   Cond. No.                         1.08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "model = sm.OLS(y, X)\n",
    "results = model.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-backup",
   "metadata": {},
   "source": [
    "MLE provides as a **point estimate of the model parameters** (in this case, the intercept `const` and slope `x1`) and **confidence intervals** (based on the standard error). I always find the interpretation of such intervals, though, a bit tricky: The idea is that, **if we were to repeat the experiment** x times, the point estimates of the parameters would fall 95% of the times in this interval.\n",
    "\n",
    "In Bayesian statistics, we are interested instead in the **uncertainty around the parameters, given a fixed set of data**. In other word, we want a **probability distribution** of the model parameters, after seeing the data (the posterior probability, see below). This is not something we can get from MLE!\n",
    "\n",
    "Here is where Bayesians and Frequentists' views mostly differ: \n",
    "- Frequentists look at fixed estimated parameters in hypothetical repeated experiments;\n",
    "- Bayesians look at the parameter distributions after a fixed set of data;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-sympathy",
   "metadata": {},
   "source": [
    "## Prior and posterior distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-dressing",
   "metadata": {},
   "source": [
    "In Bayesian parameter estimation, we express most things in terms of **parameter distributions**.\n",
    "\n",
    "First of all, we have **prior distributions**. As the name suggests, prior distributions express the uncertainty around a model's parameters **BEFORE** seeing the data. Why is this even important?\n",
    "\n",
    "By \"comparing\" prior beliefs to new observations, we have a feeling of how surprising they are: \n",
    "- When a new observation (e.g., I see a white swan) matches my prior beliefs (e.g., most swans are white) I am strenghtening my prior beliefs: The more new observations (e.g., the more white swans I see), the stronger my prior beliefs become. \n",
    "- However, when a new observation (e.g., I see a black swan) does not match my prior beliefs, this should descrease the certainty of my prior beliefs: The more new non-matching observations I collect (e.g., the more black swans I see), the weaker my prior beliefs become. Crucially, the stronger the prior beliefs, the more non-matching observations I should collect in order to change them.\n",
    "\n",
    "Therefore, from a Bayesian perspective, the uncertainty around a model's parameters **AFTER** seeing the data (aka the **posterior distributions**) has to include prior beliefs as well.\n",
    "\n",
    "Prior beliefs are also important in all situations in which we need to constantly update the model parameters based on new, incoming information. For example, in adaptive experimental designs when we want to set a certain difficulty level so that each participants will reach e.g. 60% accuracy. Or if we are replicating an experiment with a new group of participants. In these cases, we want to integrate our prior beliefs with the new information to update our posterior beliefs.\n",
    "\n",
    "If this sounds very intuitive to you, it is no coincidence, as there are different psychological theories that show similarities between Bayesian updating and different process of adaptation in the human brain! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-miniature",
   "metadata": {},
   "source": [
    "## Bayes Theorem and likelihood functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-camping",
   "metadata": {},
   "source": [
    "These principles are formalized into the [Bayes Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem):\n",
    "$$\n",
    "P(A|B) = \\frac{P(A) P(B|A)}{P(B)}\n",
    "$$\n",
    "where A and B are events and $P(B)$ is not 0.\n",
    "\n",
    "In the case of parameter estimation:\n",
    "$$\n",
    "P(\\theta|\\textbf{D}) = \\frac{P(\\theta) P(\\textbf{D} |\\theta)}{P(\\textbf{D})}\n",
    "$$\n",
    "where:\n",
    "\n",
    "- $P(\\theta|\\textbf{D})$ is the posterior probability of the parameter set $\\theta$ after observing the data $\\textbf{D}$\n",
    "- $P(\\textbf{D} |\\theta)$ is the likelihood of the data given $\\theta$\n",
    "- $P(\\theta)$ is the prior probability of the parameter set $\\theta$\n",
    "- and $P(\\textbf{D})$ is the probability of the data irrespective of $\\theta$\n",
    "\n",
    "While $P(\\theta|\\textbf{D})$ and $P(\\textbf{D} |\\theta)$ are conditional probabilities, $P(\\theta)$ and $P(\\textbf{D})$ are marginal probabilities.\n",
    "\n",
    "For our purposes, we will only focus on priors and likelihood when specifying our models, and we will inspect posterior distributions to inspect the models fit to the data.\n",
    "\n",
    "The likelihood function is something very specific of a certain model. Most things you have learned about likelihoods in the frequentist domain are also relevant here.\n",
    "\n",
    "While \"likelihood-free\" Bayesian methods are possible (i.e., for models without a closed-form solution), we will only focus here on models of which we know their likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-column",
   "metadata": {},
   "source": [
    "## Posterior predictive distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "superb-rhythm",
   "metadata": {},
   "source": [
    "When assessing the fit of a model, we are often interested in:\n",
    "1. absolute model fit: how well could the model capture the patterns we observed in the data?\n",
    "2. relative model fit: how well could the model capture the data-patterns compared to another model?\n",
    "3. how well does the model generalizes to \"new\" data (out of the sample it was used to fit the model in the first place)?\n",
    "\n",
    "**Posterior predictive distributions** are useful to reach all these goals.\n",
    "\n",
    "Once we have obtained the posterior distribution $P(\\theta|\\textbf{D})$, we have an idea of which parameter values are more likely (given the observed data $\\textbf{D}$). Therefore, if we sample $\\theta$ values, we can generate different data sets that will tell us which data are likely under our new model.\n",
    "\n",
    "For example, we estimate the average height of students in class A in a school, using a simple Gaussian distribution. We then:\n",
    "\n",
    "1. want to assess how well our model captures the data. In case, for example, we know that there were a subset of very tall students in the class, what we can do is sample different mean values from the posterior distribution, and for each of them generate data using a Gaussian distribution with that mean. For each of these generated data sets, we calculate the .9 height quantile, so that we have a distribution of .9 height quantiles. At this point we can compare the generated data summary statistics to the one observed in the data the model was fit on. Since the Gaussian model only has 1 mean, this summary statistic will likely not match the one of the data. This is an example of bad absolute model fit.\n",
    "\n",
    "2. now, we fit an alternative model, that assumes two means, one for the \"tall students\" sungroup, and one for everyone else. This model's posterior predictive distribution is more likely to match the observed data pattern. We can thus use this as evidence that this alternative model is better than the simple one-mean model.\n",
    "\n",
    "3. now, we want to know how well this model generalizes to the data of students heights in class B of the same school. Instead of fitting the model again to the data in class B, we can calculate the average heigh and .9 height quantile in class B and use the posterior predictives of the previous models to compare it to them. What do you expect to observe?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
